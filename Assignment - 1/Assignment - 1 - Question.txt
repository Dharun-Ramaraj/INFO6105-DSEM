ML Data Cleaning and Feature Selection

In this assignment, you will use a dataset for predictive learning and check the quality of the data and determine which features are important.

Answer the following questions:

* What are the data types? (Only numeric and categorical)

* Are there missing values?

* What are the likely distributions of the numeric variables?

* Which independent variables are useful to predict a target (dependent variable)? (Use at least three methods)

* Which independent variables have missing data? How much? 

* Do the training and test sets have the same data?

* In the predictor variables independent of all the other predictor variables?

* Which predictor variables are the most important?

* Do the ranges of the predictor variables make sense?

* What are the distributions of the predictor variables?   

* Remove outliers and keep outliers (does if have an effect of the final predictive model)?

* Remove 1%, 5%, and 10% of your data randomly and impute the values back using at least 3 imputation methods. How well did the methods recover the missing values?  That is remove some data, check the % error on residuals for numeric data and check for bias and variance of the error.

For categorical data, calculate the accuracy and a confusion matrix.

 

There is an example notebook for assignment one here  https://github.com/aiskunks/YouTube/blob/main/A_Crash_Course_in_Statistical_Learning/ML_Data_Cleaning_and_Feature_Selection/ML_Data_Cleaning_and_Feature_Selection_Abalone.ipynb

Imputation Methods for Missing Data
https://www.youtube.com/watch?v=fYhr8eF1ubo

Nice EDA notebook https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-pythonLinks to an external site.

Scoring Rubric

Data-Supported Answers (20 Points)

1. Are your answers substantiated with relevant data?
2. You should include tables, graphs, and charts to support your evaluation/answers.

Google Colab Compatibility (5 Points)

1.Ensure your project is compatible with Google Colab.
2.Save and submit the Google Colab notebook as a .ipynb file on Canvas.

Selection and Approval of Public Dataset (5 Points)

1.Choose a public dataset suitable for Regression or Classification.
2.Obtain approval for your chosen dataset from the TAs.

Code Originality and Citation (5 Points)

1.Clearly distinguish between your original code and any adapted sources.
2.Failure to appropriately cite any code will result in zero points for this section.

Code Explanation and Professional (20 Points)

1.Your explanation of the code should be clear and concise during the code review. 
2.Also make sure to include explanation in the notebook as well.
3.The clarity of your code review will be evaluated and scaled from 0 to 10 for scoring.

Licensing Clarification (5 Points)

1.Clearly state and cite the license under which your work is released.
2.Neglecting to cite a clear license will lead to zero points for this section.

Comprehensive Answers to Key Questions (40 Points)

1.Answer all the questions thoroughly which are mentioned in the requirements of the assignment.

 

Note: 80% of the grade will be whether the submission meets the requirements of the assignment and 20% of the assignment will depend upon how good you are able to explain the code and for professionalism.

 

Notes:

Normality - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.

Homoscedasticity - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013)Links to an external site.. Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.

Linearity- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.

Absence of correlated errors - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.